{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing, or NLP, is a field at the intersection of computer science, artificial intelligence, and linguistics. Its goal is to enable computers to understand, interpret, generate, and respond to human language in a valuable way. This understanding could range from simple tasks (like identifying the language of the text) to complex ones (like sentiment analysis, machine translation, and summarization).\n",
        "\n",
        "# What will you gain from this assignment?\n",
        "\n",
        "* **Practical Skills**: You'll gain hands-on experience working with text data — the largest data type in the world today\n",
        "\n",
        "* **Analytical Thinking**: You'll learn how to approach language as a data scientist, breaking down sentences into tokens, extracting meaningful features, and turning text into data that machines can understand\n",
        "\n",
        "* **Understanding of Core Concepts**: NLP is built on key principles of linguistics, computer science, and machine learning. Through this assignment, you'll get a sneak-peek into these fascinating areas\n",
        "\n",
        "* **Tool Familiarity**: This assignment will familiarize you with essential libraries for NLP. These tools are widely used in academia and industry, making this knowledge incredibly transferable\n",
        "\n",
        "# Note\n",
        "\n",
        "Both the test and the Jupiter notebook are required parts of this assignment\n"
      ],
      "metadata": {
        "id": "sZU9NBYJdrcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "Run the following code cells for the correct execution of the code. Also it will connect your Google Drive with the temporary storage connected with this nothebook"
      ],
      "metadata": {
        "id": "9QLQ-4rShOww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_l5Fq66dhF7"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install gensim\n",
        "!pip install wordcloud\n",
        "!pip install matplotlib\n",
        "!pip install transformers\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from wordcloud import WordCloud\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, pipeline\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload text.txt to Colab and check if path to the file is correct:"
      ],
      "metadata": {
        "id": "uqx423k9iwV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"./Ivanhoe.txt\"\n",
        "with open(data_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(min(10, len(lines))):\n",
        "        print(lines[i].strip())"
      ],
      "metadata": {
        "id": "z1psKaCuiv9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you see the following text...\n",
        "\n",
        "\n",
        "\n",
        "> ﻿The Project Gutenberg eBook of Ivanhoe: A Romance\\\n",
        "This ebook is for the use of anyone anywhere in the United States and\n",
        "most other parts of the world at no cost and with almost no restrictions\n",
        "whatsoever. You may copy it, give it away or re-use it under the terms\n",
        "of the Project Gutenberg License included with this ebook or online\n",
        "at www.gutenberg.org. If you are not located in the United States,\n",
        "you will have to check the laws of the country where you are located\n",
        "before using this eBook.\n",
        "\n",
        "... then everything is correct!"
      ],
      "metadata": {
        "id": "CXf7P83SmKQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the last cell before you will be ready to solve some real NLP problems!"
      ],
      "metadata": {
        "id": "dtsvJmmgntcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_document(path):\n",
        "    with open(path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "text = read_document(data_path)"
      ],
      "metadata": {
        "id": "LrVdytDgnszz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks"
      ],
      "metadata": {
        "id": "hOKyuVQ6m49V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1 (7 points)\n",
        "Textual data can often be overwhelming due to its volume and complexity. Visualization techniques, such as generating a word cloud, provide a quick and insightful way to understand the essential features of a text document. It is a powerful tool for exploratory data analysis, summarizing large chunks of text data in a visually appealing and interpretable manner\n",
        "\n",
        "Run the following code cell to understand how word cloud looks like"
      ],
      "metadata": {
        "id": "JFqYgTI-nCEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(background_color = 'white', width = 800, height = 400, max_words = 100, contour_width = 3, contour_color = 'steelblue')\n",
        "wordcloud.generate(text)\n",
        "\n",
        "plt.figure(figsize = (10, 10))\n",
        "plt.imshow(wordcloud, interpolation ='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X5rEY6SrmOlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**: You've just generated and seen a word cloud based on the text document. Now, let's delve into what a word cloud represents and your impressions of this particular one. In your own words, explain what a word cloud is and what it is commonly used for"
      ],
      "metadata": {
        "id": "o1gkXDA6p0ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "mmlZ9QCMqTAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 (7 points)\n",
        "Understanding the frequency distribution of words in a document provides key insights into its thematic focus and content. It can serve as a rudimentary form of text summarization and aids in grasping the essence of the document.\n",
        "\n",
        "This task is particularly important because word frequency counts are often a starting point for more advanced NLP techniques, such as text classification, clustering, or topic modeling\n",
        "\n",
        "Output the first 10 frequent words from the text"
      ],
      "metadata": {
        "id": "hbA2PHJ36c-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "word_counts = {}\n",
        "\n",
        "N = # YOUR CODE\n",
        "\n",
        "for word in words:\n",
        "    if word in word_counts:\n",
        "        word_counts[word] += 1\n",
        "    else:\n",
        "        word_counts[word] = 1\n",
        "\n",
        "sorted_word_counts = sorted(word_counts.items(), key = lambda x: x[1], reverse = True)[ : N]\n",
        "\n",
        "print(\"The  most frequent words are:\")\n",
        "for word, freq in sorted_word_counts:\n",
        "    print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "id": "SD0vnJaW_r6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is pretty surprising, isn't it?\n",
        "\n",
        "The word cloud automatically filters out \"stop words\" like *the*, *a*, *in*, and so on. These are common words that appear frequently in almost all text but are usually considered to be of little value in text analysis because they don’t carry significant meaning. Therefore, the word cloud focuses on displaying the more \"interesting\" or unique words in the text.\n",
        "\n",
        "On the other hand, when we count word frequencies manually without filtering out these stop words, they naturally rise to the top as the most frequent words. So don't worry if the two don't match; it's all about what is being filtered out!"
      ],
      "metadata": {
        "id": "uJkVSIqHWqSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: Given that the most frequent words are often stop words that might not carry much individual meaning, what other methods could we use to better understand the thematic content of the text? How might we modify our approach to get a clearer picture of the document's main topics or sentiments?"
      ],
      "metadata": {
        "id": "tZm4W_aWaLCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "EcjnvQJsbOKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 (7 points)\n",
        "In previous tasks, we focused on visualizing and identifying the most frequent words in the text. Now let's shift our focus to a single, specific word. Your task is to find all occurrences of a  word *Ivanhoe* in the text"
      ],
      "metadata": {
        "id": "SCfhpqyoYqC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "target_word = '' # YOUR CODE\n",
        "target_word_frequency = 0\n",
        "for word in words:\n",
        "    if word == target_word:\n",
        "        target_word_frequency += 1\n",
        "\n",
        "print(f\"The word '{target_word}' appears {target_word_frequency} times in the text.\")\n"
      ],
      "metadata": {
        "id": "-BDau9B8qWGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**: Now that you know how many times the word *Ivanhoe* appears in the text, what might be the significance or role of this character in the story? How does the frequency of this word correlate with its importance in the text? Would you expect this word to appear more or less frequently, and why?"
      ],
      "metadata": {
        "id": "ZZiQAvOXbx5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "hFa_NK5Hb-C7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4 (7 points)\n",
        "Text data is inherently categorical and must be converted into numerical format to be utilized by machine learning algorithms. One simple yet powerful method to achieve this is binary vectorization. In this task, you will get hands-on experience with this concept, which will lay the groundwork for more complex natural language processing tasks you'll encounter later.\n",
        "\n",
        "Convert a set of sentences into binary vectors using Python. Each position in the vector corresponds to a unique word in the vocabulary created from the selected sentences. A position in the vector will be marked as 1 if the corresponding word is present in the sentence, and 0 otherwise.\n",
        "\n",
        "**Your task is to fill the gaps in the code cell with proper values**"
      ],
      "metadata": {
        "id": "RKVnmA7mBFL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_all = text.split('. ')\n",
        "\n",
        "random_sentences = random.sample(sentences_all, 10)\n",
        "for i in range(len(random_sentences)):\n",
        "  print(\"\\nSENTENCE # \", i, \",\", random_sentences[i], \"\\n\")\n",
        "\n",
        "vocabulary = set()\n",
        "# Tokenize sentences and build vocabulary\n",
        "tokenized_sentences = []\n",
        "for sentence in random_sentences:\n",
        "    words = sentence.lower().split()\n",
        "    tokenized_sentences.append(words)\n",
        "    for word in words:\n",
        "        vocabulary.add(____) # REPLACE ___ WITH YOUR CODE\n",
        "\n",
        "vocabulary_list = list(vocabulary)\n",
        "\n",
        "# Create binary vectors\n",
        "binary_vectors = []\n",
        "for sentence in tokenized_sentences:\n",
        "    vec = []\n",
        "    for vocab_word in vocabulary_list:\n",
        "        if vocab_word in sentence:\n",
        "            vec.append(__) # REPLACE ___ WITH YOUR CODE\n",
        "        else:\n",
        "            vec.append(__) # REPLACE ___ WITH YOUR CODE\n",
        "    binary_vectors.append(vec)\n",
        "\n",
        "print(\"Binary Vectors:\")\n",
        "for vec in binary_vectors:\n",
        "    print(vec)\n"
      ],
      "metadata": {
        "id": "akq2Rpsfb8qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Astonishing result, isn't it? But let's be sure that everything is clear:\n",
        "\n",
        "\n",
        "In the given output, each `SENTENCE # X` line represents one of the 10 randomly selected sentences from the original text. Following each sentence is its corresponding binary vector, which is a list of zeros and ones. The length of each binary vector is equal to the total number of unique words in the vocabulary built from the 10 sentences.\n",
        "\n",
        "Each position in the binary vector corresponds to a specific word in the vocabulary list. The value at that position will be '1' if that specific word appears in the sentence, and '0' otherwise.\n",
        "\n",
        "Here's a simplified example to illustrate:\n",
        "\n",
        "1. Let's say the vocabulary has only four unique words: `[\"apple\", \"orange\", \"banana\", \"grape\"]`.\n",
        "2. And you have a sentence: `I like apple and banana`.\n",
        "3. The binary vector for this sentence would be `[1, 0, 1, 0]`.\n",
        "* The first position corresponds to `apple`, which is in the sentence, so the first value is `1`.\n",
        "* The second position corresponds to `orange`, which is not in the sentence, so the second value is `0`.\n",
        "* The third position corresponds to `banana`, which is in the sentence, so the third value is `1`.\n",
        "* The fourth position corresponds to `grape`, which is not in the sentence, so the fourth value is `0`.\n",
        "\n",
        "\n",
        "This binary representation is a very basic form of text vectorization, and it allows you to translate textual information into a format that machine learning algorithms can understand. It's a starting point for many more complex methods in natural language processing."
      ],
      "metadata": {
        "id": "cmPCfOnCKRCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**:\n",
        "What information is lost when we represent sentences as binary vectors?\n",
        "Can you think of a real-world application where binary vectorization would be particularly useful?"
      ],
      "metadata": {
        "id": "1VWLvtPALfEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "ZNfjKQ9OLryj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5 (7 points)\n"
      ],
      "metadata": {
        "id": "dmEPy2WjLwQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Word2Vec is a more advanced technique that represents each word in a continuous vector space. This method captures semantic relationships between words, unlike simple binary vectorization, which simply shows the presence or absence of a word in a sentence\n",
        "\n",
        "\n",
        " So, in simple words, we match each word with its numerical value, which indicates its proximity to all other words words"
      ],
      "metadata": {
        "id": "FaAebE4g_Lct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text.lower().translate(str.maketrans('', '', string.punctuation)).split('.')\n",
        "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
        "\n",
        "vector_size = 100  # Dimensionality of the word vectors. You can change this and see what happens\n",
        "window_size = 5\n",
        "\n",
        "word2vec_model = Word2Vec(sentences = tokenized_sentences, vector_size = vector_size, window = window_size, sg = 0, min_count = 1)\n",
        "\n",
        "word2vec_model.train(tokenized_sentences, total_examples = len(tokenized_sentences), epochs=10)\n",
        "\n",
        "# Find the vector representation for specific words\n",
        "specific_words = ['ivanhoe', 'cedric']\n",
        "for word in specific_words:\n",
        "    try:\n",
        "        vector = word2vec_model.wv[word]\n",
        "        print(f\"The vector representation for the word '{word}' is {vector}\")\n",
        "    except KeyError:\n",
        "        print(f\"The word '{word}' does not exist in the vocabulary.\")\n",
        "\n",
        "# Find the most similar words to a specific word\n",
        "try:\n",
        "    similar_words = word2vec_model.wv.most_similar('ivanhoe', topn=5)\n",
        "    print(f\"The most similar words to 'ivanhoe' are: {similar_words}\")\n",
        "except KeyError:\n",
        "    print(\"The word 'ivanhoe' does not exist in the vocabulary.\")\n"
      ],
      "metadata": {
        "id": "68kAqOWb0iWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You see that vector representation of the word is as big as binary vectorization of the whole sentence! However, with bigger dimensional comes bigger precision: in natural language processing, representing words as vectors in a high-dimensional space is a common technique to capture semantic and syntactic information about the words. When we say that the vector representation for a word is \"big,\" we usually refer to the number of dimensions the vector has.\n",
        "\n",
        "The more dimensions you have, the easier it is to distinguish between words. When you have a large vocabulary, being able to separate words distinctly in the vector space is crucial for tasks like classification, clustering, or similarity measurement.\n",
        "\n",
        "**Now it is your turn to find the most similar word to the word** *knight*"
      ],
      "metadata": {
        "id": "YYungrmX84zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = ' ' # YOUR CODE\n",
        "try:\n",
        "    similar_words = word2vec_model.wv.most_similar(word, topn=5)\n",
        "    print(f\"The most similar words to 'knight' are: {similar_words}\")\n",
        "except KeyError:\n",
        "    print(\"The word 'knight' does not exist in the vocabulary.\")\n"
      ],
      "metadata": {
        "id": "HgR-bo6qCMRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**: What are the benefits and limitations of using a high-dimensional space for word vectors?"
      ],
      "metadata": {
        "id": "3hR-zBc0DoCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "AsAHbA6xDryi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6 (7 points)\n",
        "\n"
      ],
      "metadata": {
        "id": "IVkRMx-XDt65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task of finding synonyms and antonyms in text is crucial for several reasons, spanning various fields like natural language processing, linguistics, and even cognitive science.\n",
        "\n",
        "The identification of synonyms and antonyms is not merely a lexical exercise but a task that holds significant implications for both machine and human understanding of language. It is a foundational task in NLP and continues to be an area of active research and application.\n",
        "\n",
        "**Your task is to find synonyms and antonyms of the word** *happy*"
      ],
      "metadata": {
        "id": "RjRilO07HnOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "sentences = text.split('.')\n",
        "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
        "\n",
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1)\n",
        "\n",
        "def find_antonyms_by_negation(model, target_word, topn=10):\n",
        "    try:\n",
        "        neg_vector = -model.wv[target_word]\n",
        "        similar_to_neg = model.wv.similar_by_vector(neg_vector, topn=topn)\n",
        "        return similar_to_neg\n",
        "    except KeyError:\n",
        "        return f\"{target_word} is not in vocabulary\"\n",
        "\n",
        "target_word = # YOUR CODE HERE\n",
        "try:\n",
        "    similar_words = model.wv.most_similar(target_word, topn=10)\n",
        "    print(f\"Synonyms to word '{target_word}' in the context of the text: {similar_words}\")\n",
        "except KeyError:\n",
        "    print(f\"'{target_word}' is not in vocabulary\")\n",
        "\n",
        "similar_to_neg = find_antonyms_by_negation(model, target_word)\n",
        "print(f\"Antonyms to word '{target_word}' in the context of the text: {similar_to_neg}\")\n"
      ],
      "metadata": {
        "id": "cTCMgkyJEbeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concepts of synonyms and antonyms are not as straightforward as they may appear because language is deeply context-dependent. Words that are synonyms or antonyms in one context may not hold the same relationship in another setting.\n",
        "\n",
        "Because of these complexities, it's crucial to consider context when identifying synonyms and antonyms. In some cases, they can be tagged as such only within the specific text being analyzed."
      ],
      "metadata": {
        "id": "rkWmoH85IrUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** How would you handle words that have multiple meanings in the task of identifying synonyms and antonyms?"
      ],
      "metadata": {
        "id": "woHZ5-_OKbHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "fiYQH_eEKqgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7 (9 points)\n",
        "\n",
        "Summarization aims to reduce the content to its most essential points, delivering the same message but in a more concise manner. This is particularly useful for quickly understanding large volumes of text or identifying the most important information within a document.\n",
        "\n",
        "In this task, you'll focus on summarizing a randomly selected sentence from a given text."
      ],
      "metadata": {
        "id": "aLJ-LS8_KuyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "summarizer = pipeline(\"summarization\", model = model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "TZDJC9s3MDEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text.split('. ')\n",
        "sentences = [s.strip() for s in sentences if s]\n",
        "\n",
        "random_sentence = random.choice([s for s in sentences if len(s) >= 30])\n",
        "summary = summarizer(random_sentence, max_length = 50, min_length = 5, do_sample = False)\n",
        "\n",
        "print(\"Original Sentence:\", random_sentence)\n",
        "print(\"Summary:\", summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "8TEXDWKKLcmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** How well did the machine-generated summary capture the essence of the original text? What important details were omitted in the machine summary? What improvements would you suggest for the current summarization model?"
      ],
      "metadata": {
        "id": "AeLjT_aPNwQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "NuGzk4y3N2xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus task (bonus 10 points)\n",
        "\n",
        "The goal is to build a simple sentiment analyzer that categorizes the sentences from your original text as either 'positive' or 'negative'.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1. **Feature Extraction**: Utilize one of the vectorization techniques you've previously learned to turn your sentences into numerical data. This could be binary representation or Word2Vec.\n",
        "\n",
        "3. **Model Training**: Use a straightforward machine learning model like Naive Bayes to train on this small dataset.\n",
        "\n",
        "4. **Prediction**: Use the model to predict the sentiment of remaining sentences in the text. Display the sentences along with their predicted sentiments."
      ],
      "metadata": {
        "id": "QvljuWe8PCTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Randomly select 1000 sentences for training\n",
        "train_sentences = # YOUR CODE HERE\n",
        "\n",
        "labels = [random.choice([0, 1]) for _ in range(len(train_sentences))]\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = # YOUR CODE HERE\n",
        "X_train = # YOUR CODE HERE\n",
        "\n",
        "# Train a simple classifier\n",
        "clf = # YOUR CODE HERE\n",
        "\n",
        "# Randomly select a sentence for testing\n",
        "random_sentence = # YOUR CODE HERE\n",
        "\n",
        "# Make predictions\n",
        "X_test = # YOUR CODE HERE\n",
        "y_pred = # YOUR CODE HERE\n",
        "\n",
        "# Map label to sentiment\n",
        "sentiment = 'positive' if y_pred[0] == 1 else 'negative'\n",
        "\n",
        "print(f\"Sentence: {________}\") # YOUR CODE HERE\n",
        "print(f\"Predicted Sentiment: {_________}\") # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "HuTtldH4PBj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Can you explain what text vectorization is and why it's necessary for NLP tasks? Why is it important to split the data into training and test sets?"
      ],
      "metadata": {
        "id": "9a2rlVHyS2T4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "n1ZzFBUxTBY0"
      }
    }
  ]
}